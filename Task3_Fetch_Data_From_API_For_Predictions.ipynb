{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNELw5AI3/6Wv2JIlnuRlUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakar-ahmed/Formative_1_Databases/blob/main/Task3_Fetch_Data_From_API_For_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1DDXtuNh320",
        "outputId": "4acab785-ca57-4c85-a0be-a44dc651b9b8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2SGeynrhiXW",
        "outputId": "019616a4-e917-40bb-e5b6-1d7f5a4e99a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fetched successfully!\n",
            "   Diagnosis  Disease_Duration  Hospitalizations  Family_History  \\\n",
            "0          2                 4                 2               2   \n",
            "1          1                 5                 2               0   \n",
            "2          1                 5                 2               0   \n",
            "3          1                 5                 2               0   \n",
            "4          1                 5                 2               0   \n",
            "\n",
            "   Substance_Use  Suicide_Attempt  Positive_Symptom_Score  \\\n",
            "0              2                2                      50   \n",
            "1              1                0                      45   \n",
            "2              1                0                      45   \n",
            "3              1                0                      45   \n",
            "4              1                0                      45   \n",
            "\n",
            "   Negative_Symptom_Score  GAF_Score  Patient_ID  \n",
            "0                      40         60           2  \n",
            "1                      70         68           3  \n",
            "2                      30         56           4  \n",
            "3                      30         65           1  \n",
            "4                      30         65           6  \n",
            "Not enough samples for SMOTE. Proceeding without resampling.\n",
            "Stratified split failed due to insufficient samples in one of the classes. Falling back to random split.\n",
            "Reloading Tuner from my_dir/schizophrenia_diagnosis/tuner0.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model retrieved successfully.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531ms/step - accuracy: 1.0000 - loss: 0.1097\n",
            "Test Accuracy: 100.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [0 0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       0.50      0.50      0.50         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Model saved to best_trained_model.keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "Predicted Diagnosis: 1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras import layers, regularizers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from keras_tuner import HyperModel\n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "\n",
        "# --- Step 1: Fetch Data from API ---\n",
        "api_url = \"https://formative-1-databases.onrender.com//medical-history/\"\n",
        "response = requests.get(api_url)\n",
        "\n",
        "# Process if data is fetched successfully\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Data fetched successfully!\")\n",
        "    print(df.head())\n",
        "\n",
        "    features = [\n",
        "        'Disease_Duration', 'Family_History', 'Substance_Use',\n",
        "        'Suicide_Attempt', 'Positive_Symptom_Score',\n",
        "        'Negative_Symptom_Score', 'GAF_Score'\n",
        "    ]\n",
        "    target = 'Diagnosis'\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    # Encode target\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # --- Apply SMOTE Oversampling with adaptive k_neighbors ---\n",
        "    min_class_count = np.min(np.bincount(y_encoded))\n",
        "    if min_class_count > 1:\n",
        "        k_neighbors = min(5, min_class_count - 1)\n",
        "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
        "    else:\n",
        "        print(\"Not enough samples for SMOTE. Proceeding without resampling.\")\n",
        "        X_resampled, y_resampled = X, y_encoded\n",
        "\n",
        "    # Train-test split (modified to check for class count)\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled, shuffle=True\n",
        "        )\n",
        "    except ValueError:\n",
        "        print(\"Stratified split failed due to insufficient samples in one of the classes. Falling back to random split.\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_resampled, y_resampled, test_size=0.2, random_state=42, shuffle=True\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Calculate Class Weights ---\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_resampled),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    # --- Step 3: Define HyperModel ---\n",
        "    class MyHyperModel(HyperModel):\n",
        "        def build(self, hp):\n",
        "            model = models.Sequential()\n",
        "            model.add(layers.Input(shape=(X_train.shape[1],)))\n",
        "            model.add(layers.Dense(\n",
        "                units=hp.Int('units', min_value=32, max_value=128, step=16),\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01)\n",
        "            ))\n",
        "            model.add(layers.BatchNormalization())\n",
        "            model.add(layers.Dropout(0.5))\n",
        "            model.add(layers.Dense(64, activation='relu'))\n",
        "            model.add(layers.Dense(len(np.unique(y_resampled)), activation='softmax'))\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            return model\n",
        "\n",
        "    # --- Step 4: Hyperparameter Tuning ---\n",
        "    tuner = RandomSearch(\n",
        "        MyHyperModel(),\n",
        "        objective='val_accuracy',\n",
        "        max_trials=5,\n",
        "        executions_per_trial=1,\n",
        "        directory='my_dir',\n",
        "        project_name='schizophrenia_diagnosis'\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stop], class_weight=class_weights)\n",
        "\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    print(\"Best model retrieved successfully.\")\n",
        "\n",
        "    # --- Step 5: Evaluate and Save the Model ---\n",
        "    test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred, labels=np.unique(y_resampled)))\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, labels=np.unique(y_resampled)))\n",
        "\n",
        "    best_model.save('best_trained_model.keras')\n",
        "    print('Model saved to best_trained_model.keras')\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "    print(response.text)\n",
        "    df = None\n",
        "\n",
        "# --- Step 6: Load Model and Make Predictions\n",
        "def make_prediction(input_data):\n",
        "    if not os.path.exists('best_trained_model.keras'):\n",
        "        print(\"Model file not found. Skipping prediction.\")\n",
        "        return None\n",
        "\n",
        "    model = load_model('best_trained_model.keras')\n",
        "    input_array = np.array([input_data])\n",
        "    prediction = model.predict(input_array)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    return predicted_class\n",
        "\n",
        "# Example prediction only if model exists\n",
        "if os.path.exists('best_trained_model.keras'):\n",
        "    example_input = [3, 1, 1, 0, 15, 40, 72]\n",
        "    predicted = make_prediction(example_input)\n",
        "\n",
        "    if predicted is not None:\n",
        "        predicted_label = label_encoder.inverse_transform(predicted)\n",
        "        print(f\"Predicted Diagnosis: {predicted_label[0]}\")\n",
        "    else:\n",
        "        print(\"Prediction skipped due to missing model.\")\n",
        "else:\n",
        "    print(\"No trained model available. Skipping prediction.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Step 1: Load the pre-trained model\n",
        "model = load_model('best_trained_model.keras')\n",
        "\n",
        "# Step 2: Load the data you want to predict\n",
        "X_to_predict = patients_to_predict.drop(columns=['Patient_ID'])  # Drop the Patient_ID or other non-feature columns\n",
        "\n",
        "# Step 3: Ensure the input data for prediction matches the shape used in training\n",
        "\n",
        "print(f\"Shape of X_to_predict before ensuring consistency: {X_to_predict.shape}\")\n",
        "X_to_predict = X_to_predict.iloc[:, :7]\n",
        "print(f\"Shape of X_to_predict after consistency check: {X_to_predict.shape}\")\n",
        "\n",
        "# Step 4: Make predictions for the selected patients\n",
        "predictions = model.predict(X_to_predict)\n",
        "\n",
        "# Step 5: Convert the predictions to binary labels (0 or 1)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Step 6: Map predictions to human-readable labels (\"No Schizophrenia\" or \"Schizophrenia\")\n",
        "predicted_labels = np.where(predicted_classes == 0, \"No Schizophrenia\", \"Schizophrenia\")\n",
        "\n",
        "# Step 7: Add predictions to the original DataFrame\n",
        "patients_to_predict['Predicted_Diagnosis'] = predicted_labels\n",
        "\n",
        "# Step 8: Show the predictions\n",
        "print(patients_to_predict[['Patient_ID', 'Predicted_Diagnosis']])\n",
        "\n",
        "# Optional: Save the predictions to a new CSV file\n",
        "patients_to_predict.to_csv('patients_with_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-ETcVbrDpp",
        "outputId": "c5c93656-bbee-4bd7-a1c8-bfdf2b563576"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_to_predict before ensuring consistency: (5, 10)\n",
            "Shape of X_to_predict after consistency check: (5, 7)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
            "   Patient_ID Predicted_Diagnosis\n",
            "0           2       Schizophrenia\n",
            "1           3       Schizophrenia\n",
            "2           4       Schizophrenia\n",
            "3           1       Schizophrenia\n",
            "4           6       Schizophrenia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get the raw predicted probabilities (instead of class labels)\n",
        "predicted_probabilities = model.predict(X_to_predict)\n",
        "\n",
        "# Step 2: Show the probabilities for each patient\n",
        "print(\"Predicted Probabilities (for Schizophrenia):\")\n",
        "print(predicted_probabilities)\n",
        "\n",
        "# Step 3: Convert probabilities to labels (0 for No Schizophrenia, 1 for Schizophrenia)\n",
        "predicted_classes = (predicted_probabilities > 0.5).astype(int)\n",
        "\n",
        "# Step 4: Map predictions to human-readable labels (\"No Schizophrenia\" or \"Schizophrenia\")\n",
        "predicted_labels = np.where(predicted_classes == 0, \"No Schizophrenia\", \"Schizophrenia\")\n",
        "\n",
        "# Step 5: Add predictions to the original DataFrame\n",
        "patients_to_predict['Predicted_Diagnosis'] = predicted_labels\n",
        "\n",
        "# Step 6: Show the predictions\n",
        "print(patients_to_predict[['Patient_ID', 'Predicted_Diagnosis']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yH_wmSGs1qT",
        "outputId": "b21ece57-b0d4-46ae-ae09-8c7ac39574a1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Predicted Probabilities (for Schizophrenia):\n",
            "[[9.9999547e-01 4.4941012e-06]\n",
            " [9.9998200e-01 1.7951921e-05]\n",
            " [9.9998200e-01 1.7951921e-05]\n",
            " [9.9998200e-01 1.7951921e-05]\n",
            " [9.9998200e-01 1.7951921e-05]]\n",
            "   Patient_ID Predicted_Diagnosis\n",
            "0           2       Schizophrenia\n",
            "1           3       Schizophrenia\n",
            "2           4       Schizophrenia\n",
            "3           1       Schizophrenia\n",
            "4           6       Schizophrenia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Load the dataset from the provided CSV link\n",
        "url = \"https://github.com/abubakar-ahmed/Formative_1_Databases/raw/main/schizophrenia_dataset_eng_version.csv\"\n",
        "df_new = pd.read_csv(url)\n",
        "\n",
        "#Check the first few rows of the new dataset\n",
        "print(df_new.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW8dzcWE2R8l",
        "outputId": "9bb0863c-8c63-49c9-dd22-4fdf8b6e93e8"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Patient ID  Age  Gender  Education Level  Martial Status  Occupation  \\\n",
            "0           1   72       1                4               2           0   \n",
            "1           2   49       1                5               2           2   \n",
            "2           3   53       1                5               3           2   \n",
            "3           4   67       1                3               2           0   \n",
            "4           5   54       0                1               2           0   \n",
            "\n",
            "   Income level  Live Area  Diagnosis  Disease Duration  Hospitalizations  \\\n",
            "0             2          1          0                 0                 0   \n",
            "1             1          0          1                35                 1   \n",
            "2             1          0          1                32                 0   \n",
            "3             2          0          0                 0                 0   \n",
            "4             2          1          0                 0                 0   \n",
            "\n",
            "   Family History  Substance use  Suicide Attempt  Positive Symptom Score  \\\n",
            "0               0              0                0                      32   \n",
            "1               1              1                1                      51   \n",
            "2               1              0                0                      72   \n",
            "3               0              1                0                      10   \n",
            "4               0              0                0                       4   \n",
            "\n",
            "   Negative Symptom Score  GAF Score  Social Support  Stress Factors  \\\n",
            "0                      48         72               0               2   \n",
            "1                      63         40               2               2   \n",
            "2                      85         51               0               1   \n",
            "3                      21         74               1               1   \n",
            "4                      27         98               0               1   \n",
            "\n",
            "   Medication Adherence  \n",
            "0                     2  \n",
            "1                     0  \n",
            "2                     1  \n",
            "3                     2  \n",
            "4                     0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# existing DataFrame from the API and df_new is the new data from the CSV\n",
        "df_combined = pd.concat([df, df_new], ignore_index=True)\n",
        "\n",
        "# Check the combined data\n",
        "print(df_combined.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17UzWeYX9ND-",
        "outputId": "1a9fe030-8cf8-44e2-cadb-b46a8f952761"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Diagnosis  Disease_Duration  Hospitalizations  Family_History  \\\n",
            "0          2               4.0                 2             2.0   \n",
            "1          1               5.0                 2             0.0   \n",
            "2          1               5.0                 2             0.0   \n",
            "3          1               5.0                 2             0.0   \n",
            "4          1               5.0                 2             0.0   \n",
            "\n",
            "   Substance_Use  Suicide_Attempt  Positive_Symptom_Score  \\\n",
            "0            2.0              2.0                    50.0   \n",
            "1            1.0              0.0                    45.0   \n",
            "2            1.0              0.0                    45.0   \n",
            "3            1.0              0.0                    45.0   \n",
            "4            1.0              0.0                    45.0   \n",
            "\n",
            "   Negative_Symptom_Score  GAF_Score  Patient_ID  ...  Disease Duration  \\\n",
            "0                    40.0       60.0         2.0  ...               NaN   \n",
            "1                    70.0       68.0         3.0  ...               NaN   \n",
            "2                    30.0       56.0         4.0  ...               NaN   \n",
            "3                    30.0       65.0         1.0  ...               NaN   \n",
            "4                    30.0       65.0         6.0  ...               NaN   \n",
            "\n",
            "   Family History  Substance use  Suicide Attempt  Positive Symptom Score  \\\n",
            "0             NaN            NaN              NaN                     NaN   \n",
            "1             NaN            NaN              NaN                     NaN   \n",
            "2             NaN            NaN              NaN                     NaN   \n",
            "3             NaN            NaN              NaN                     NaN   \n",
            "4             NaN            NaN              NaN                     NaN   \n",
            "\n",
            "   Negative Symptom Score  GAF Score  Social Support  Stress Factors  \\\n",
            "0                     NaN        NaN             NaN             NaN   \n",
            "1                     NaN        NaN             NaN             NaN   \n",
            "2                     NaN        NaN             NaN             NaN   \n",
            "3                     NaN        NaN             NaN             NaN   \n",
            "4                     NaN        NaN             NaN             NaN   \n",
            "\n",
            "   Medication Adherence  \n",
            "0                   NaN  \n",
            "1                   NaN  \n",
            "2                   NaN  \n",
            "3                   NaN  \n",
            "4                   NaN  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df_combined.dropna(subset=['Diagnosis'])\n"
      ],
      "metadata": {
        "id": "Er_qRYWY9lRA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the column names in the dataset\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhNTTXON-UDe",
        "outputId": "947bb621-7b19-4225-92a5-c2ed13f90270"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Diagnosis', 'Disease_Duration', 'Hospitalizations', 'Family_History',\n",
            "       'Substance_Use', 'Suicide_Attempt', 'Positive_Symptom_Score',\n",
            "       'Negative_Symptom_Score', 'GAF_Score', 'Patient_ID'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the model architecture\n",
        "def build_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),  # Input layer\n",
        "        layers.Dense(32, activation='relu'),  # First hidden layer\n",
        "        layers.Dropout(0.3),  # Dropout for regularization\n",
        "        layers.Dense(16, activation='relu'),  # Second hidden layer\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')  # Output layer (for binary classification)\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Example data\n",
        "X = np.random.rand(100, 8)\n",
        "y = np.random.randint(0, 2, 100)\n",
        "\n",
        "# Normalize data for better model performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle class imbalance using class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Build and train the model\n",
        "model = build_model((X_train.shape[1],))\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=8, validation_data=(X_val, y_val), class_weight=class_weights_dict)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "X_test = np.random.rand(10, 8)\n",
        "y_test = np.random.randint(0, 2, 10)\n",
        "\n",
        "# Normalize test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4jDq8vL-0-Z",
        "outputId": "a0f0e246-f3b2-46c6-d20e-79a8a1c8e9e8"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.4737 - loss: 0.7626 - val_accuracy: 0.5000 - val_loss: 0.7920\n",
            "Epoch 2/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4302 - loss: 0.7467 - val_accuracy: 0.5000 - val_loss: 0.7824\n",
            "Epoch 3/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5749 - loss: 0.6775 - val_accuracy: 0.4500 - val_loss: 0.7837\n",
            "Epoch 4/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4910 - loss: 0.6924 - val_accuracy: 0.4500 - val_loss: 0.7839\n",
            "Epoch 5/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5043 - loss: 0.7131 - val_accuracy: 0.4500 - val_loss: 0.7775\n",
            "Epoch 6/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5444 - loss: 0.7136 - val_accuracy: 0.4500 - val_loss: 0.7753\n",
            "Epoch 7/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4689 - loss: 0.7339 - val_accuracy: 0.5000 - val_loss: 0.7731\n",
            "Epoch 8/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5481 - loss: 0.6875 - val_accuracy: 0.5500 - val_loss: 0.7724\n",
            "Epoch 9/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4868 - loss: 0.7377 - val_accuracy: 0.5500 - val_loss: 0.7740\n",
            "Epoch 10/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6070 - loss: 0.6679 - val_accuracy: 0.5500 - val_loss: 0.7763\n",
            "Epoch 11/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6065 - loss: 0.6375 - val_accuracy: 0.5500 - val_loss: 0.7778\n",
            "Epoch 12/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5401 - loss: 0.6984 - val_accuracy: 0.5000 - val_loss: 0.7793\n",
            "Epoch 13/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5676 - loss: 0.7147 - val_accuracy: 0.5000 - val_loss: 0.7814\n",
            "Epoch 14/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6311 - loss: 0.6359 - val_accuracy: 0.5000 - val_loss: 0.7835\n",
            "Epoch 15/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6609 - loss: 0.6288 - val_accuracy: 0.5000 - val_loss: 0.7848\n",
            "Epoch 16/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6492 - loss: 0.6407 - val_accuracy: 0.5000 - val_loss: 0.7836\n",
            "Epoch 17/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5718 - loss: 0.6399 - val_accuracy: 0.4500 - val_loss: 0.7853\n",
            "Epoch 18/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5270 - loss: 0.6526 - val_accuracy: 0.4000 - val_loss: 0.7861\n",
            "Epoch 19/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6374 - loss: 0.6720 - val_accuracy: 0.4000 - val_loss: 0.7877\n",
            "Epoch 20/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5910 - loss: 0.6320 - val_accuracy: 0.4500 - val_loss: 0.7938\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7000 - loss: 0.6506\n",
            "Test Accuracy: 70.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "Confusion Matrix:\n",
            "[[3 0]\n",
            " [3 4]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         3\n",
            "           1       1.00      0.57      0.73         7\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.75      0.79      0.70        10\n",
            "weighted avg       0.85      0.70      0.71        10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU8Ict5j3EDL",
        "outputId": "efc4d1e2-4c9a-445a-9e5c-5ba22722cd83"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7000 - loss: 0.6506\n",
            "Test Loss: 0.6505604386329651, Test Accuracy: 0.699999988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 5 new samples with random values\n",
        "X_new = np.random.rand(5, 8)  # 5 new samples, 8 features\n",
        "\n",
        "# Normalize the new data using the scaler\n",
        "X_new_scaled = scaler.transform(X_new)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = model.predict(X_new_scaled)\n",
        "\n",
        "# Convert predictions to binary (0 for no schizophrenia, 1 for schizophrenia)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Output the predictions\n",
        "for i, pred in enumerate(predictions_binary):\n",
        "    print(f\"Prediction for sample {i + 1}: {'Schizophrenia' if pred == 1 else 'No Schizophrenia'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ26clW-GV9b",
        "outputId": "d574dd67-52af-4660-df21-a0f9893bfe47"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "Prediction for sample 1: No Schizophrenia\n",
            "Prediction for sample 2: No Schizophrenia\n",
            "Prediction for sample 3: No Schizophrenia\n",
            "Prediction for sample 4: Schizophrenia\n",
            "Prediction for sample 5: No Schizophrenia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the CSV data from the provided URL\n",
        "url = \"https://github.com/abubakar-ahmed/Formative_1_Databases/raw/main/schizophrenia_dataset_eng_version.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Ensure the features selected match those used during training\n",
        "training_features = [\n",
        "    'Disease Duration', 'Family History', 'Substance use', 'Suicide Attempt',\n",
        "    'Positive Symptom Score', 'Negative Symptom Score', 'GAF Score', 'Social Support'\n",
        "]\n",
        "\n",
        "# Selecting only the relevant features for the new samples\n",
        "X_new = df[training_features]\n",
        "\n",
        "#  Drop rows with missing values\n",
        "X_new = X_new.dropna()\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_new_scaled = scaler.fit_transform(X_new)\n",
        "\n",
        "# Reshape the data to match the model input shape\n",
        "X_new_scaled_reshaped = X_new_scaled[-5:].reshape(-1, 8)  # 5 samples, 8 features\n",
        "\n",
        "# Make predictions on the reshaped last 5 samples\n",
        "predictions = model.predict(X_new_scaled_reshaped)\n",
        "\n",
        "# Convert the predictions to readable output\n",
        "prediction_labels = ['Schizophrenia' if p > 0.5 else 'No Schizophrenia' for p in predictions]\n",
        "\n",
        "# Display the predictions for the last 5 samples\n",
        "for i, (sample, label) in enumerate(zip(df.index[-5:], prediction_labels)):\n",
        "    print(f\"Prediction for sample {sample}: {label}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEpouXtHG77Q",
        "outputId": "8b3d6b49-b847-417b-d754-0f98cb78cdf4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Prediction for sample 9995: No Schizophrenia\n",
            "Prediction for sample 9996: Schizophrenia\n",
            "Prediction for sample 9997: No Schizophrenia\n",
            "Prediction for sample 9998: No Schizophrenia\n",
            "Prediction for sample 9999: No Schizophrenia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the CSV data from the provided URL\n",
        "url = \"https://github.com/abubakar-ahmed/Formative_1_Databases/raw/main/schizophrenia_dataset_eng_version.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Ensure the features selected match those used during training\n",
        "training_features = [\n",
        "    'Disease Duration', 'Family History', 'Substance use', 'Suicide Attempt',\n",
        "    'Positive Symptom Score', 'Negative Symptom Score', 'GAF Score', 'Social Support'\n",
        "]\n",
        "\n",
        "# Selecting only the relevant features for the new samples\n",
        "X_new = df[training_features]\n",
        "\n",
        "#  Drop rows with missing values\n",
        "X_new = X_new.dropna()\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_new_scaled = scaler.fit_transform(X_new)\n",
        "\n",
        "# Select random 5 samples from the dataset\n",
        "random_samples_scaled = X_new_scaled[np.random.choice(X_new_scaled.shape[0], 5, replace=False)]\n",
        "\n",
        "# Make predictions on the random 5 samples\n",
        "predictions = model.predict(random_samples_scaled)\n",
        "\n",
        "# Convert the predictions to readable output\n",
        "prediction_labels = ['Schizophrenia' if p > 0.5 else 'No Schizophrenia' for p in predictions]\n",
        "\n",
        "# Display the predictions for the random 5 samples\n",
        "for i, (sample, label) in enumerate(zip(df.index[np.random.choice(X_new_scaled.shape[0], 5, replace=False)], prediction_labels)):\n",
        "    print(f\"Prediction for sample {sample}: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V2m01RqI5Bc",
        "outputId": "9b829998-bcb7-4829-ca69-6de51510a408"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Prediction for sample 7790: Schizophrenia\n",
            "Prediction for sample 6977: No Schizophrenia\n",
            "Prediction for sample 153: Schizophrenia\n",
            "Prediction for sample 5523: No Schizophrenia\n",
            "Prediction for sample 1329: No Schizophrenia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNslD49VM-KH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}